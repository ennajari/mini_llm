{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e526a41-5eaa-44fd-a250-d3081c7aa40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test Encoder + Classification Sentiment ===\n",
      "English input shape: torch.Size([2, 5])\n",
      "Encoder output shape: torch.Size([2, 5, 128])\n",
      "Sentiment logits: tensor([[-0.0549,  0.0309],\n",
      "        [ 0.0787,  0.0022]], grad_fn=<AddmmBackward0>)\n",
      "Sentiment predictions: tensor([1, 0]) (0=neg, 1=pos)\n",
      "True labels: tensor([1, 0])\n",
      "✓ Encoder traite l'anglais + classe le sentiment ! (aléatoire sans entraînement)\n"
     ]
    }
   ],
   "source": [
    "# Cellule 1: Encoder avec Tête de Classification de Sentiment (basé sur TransformerEncoder, ajoute classification positive/négative)\n",
    "# Utilise les classes définies précédemment. Test sur exemples anglais simulés avec labels de sentiment.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# (Toutes les classes précédentes : scaled_dot_product_attention, MultiHeadAttention, LayerNormalization, \n",
    "# PositionWiseFeedForward, PositionalEncoding, TokenEmbedding, TransformerEncoderBlock, TransformerEncoder\n",
    "# sont assumées définies ici ou dans cellules précédentes. Copiez-les si nécessaire.)\n",
    "\n",
    "# Ajout : Tête de classification sur sortie de l'Encoder\n",
    "class SentimentClassifierHead(nn.Module):\n",
    "    def __init__(self, d_model, num_classes=2):  # 2: positive/negative\n",
    "        super().__init__()\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)  # Moyenne sur séquence\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "    \n",
    "    def forward(self, encoder_output):\n",
    "        pooled = self.pool(encoder_output.transpose(1, 2)).squeeze(-1)  # [batch, d_model]\n",
    "        logits = self.classifier(pooled)\n",
    "        return logits\n",
    "\n",
    "# Encoder étendu avec classification\n",
    "class EncoderWithClassification(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, max_seq_len=1000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(vocab_size, d_model, num_heads, d_ff, num_layers, max_seq_len, dropout)\n",
    "        self.sentiment_head = SentimentClassifierHead(d_model)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        encoder_output, _ = self.encoder(x, mask)\n",
    "        sentiment_logits = self.sentiment_head(encoder_output)\n",
    "        return encoder_output, sentiment_logits\n",
    "\n",
    "# Paramètres (vocab anglais simple pour test)\n",
    "en_vocab_size = 100  # Petit vocab pour démo\n",
    "d_model = 128\n",
    "num_heads = 4\n",
    "d_ff = 512\n",
    "num_layers = 2\n",
    "\n",
    "# Instanciation\n",
    "encoder_clf = EncoderWithClassification(en_vocab_size, d_model, num_heads, d_ff, num_layers)\n",
    "\n",
    "# Exemples simulés : Phrases anglaises tokenisées (aléatoires mais étiquetées)\n",
    "# Ex: batch=2, seq=5 ; labels: 1=positive, 0=negative\n",
    "en_sentences = torch.randint(1, en_vocab_size, (2, 5))  # Tokens anglais\n",
    "sentiments = torch.tensor([1, 0])  # Positive, Negative\n",
    "\n",
    "# Forward : Encoder output + classification\n",
    "encoder_out, sent_logits = encoder_clf(en_sentences)\n",
    "\n",
    "# Test classification (simulé, car non entraîné : aléatoire)\n",
    "sent_preds = sent_logits.argmax(dim=-1)\n",
    "print(\"=== Test Encoder + Classification Sentiment ===\")\n",
    "print(f\"English input shape: {en_sentences.shape}\")\n",
    "print(f\"Encoder output shape: {encoder_out.shape}\")\n",
    "print(f\"Sentiment logits: {sent_logits}\")\n",
    "print(f\"Sentiment predictions: {sent_preds} (0=neg, 1=pos)\")\n",
    "print(f\"True labels: {sentiments}\")\n",
    "print(\"✓ Encoder traite l'anglais + classe le sentiment ! (aléatoire sans entraînement)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6fea2b9-ca29-414d-86b9-941b2c39bc7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test Decoder + Génération Traduction ===\n",
      "French target shape: torch.Size([2, 6])\n",
      "Decoder output shape: torch.Size([2, 6, 128])\n",
      "Exemple decoder_out[0, 0, :5]: tensor([ 1.0130,  0.2200, -3.5152, -0.4402, -0.4495], grad_fn=<SliceBackward0>)\n",
      "✓ Decoder génère français à partir d'encoder anglais ! (aléatoire sans entraînement)\n"
     ]
    }
   ],
   "source": [
    "# Cellule 2: Decoder pour Génération de Traduction Français (basé sur TransformerDecoder précédent)\n",
    "# Utilise encoder_out pour cross-attention. Test sur tokens français simulés.\n",
    "\n",
    "# (Classes précédentes : create_causal_mask, CrossMultiHeadAttention, TransformerDecoderBlock, TransformerDecoder\n",
    "# assumées définies.)\n",
    "\n",
    "# Decoder inchangé, mais vocab français\n",
    "fr_vocab_size = 100  # Petit vocab français pour démo\n",
    "decoder = TransformerDecoder(fr_vocab_size, d_model, num_heads, d_ff, num_layers)\n",
    "\n",
    "# Exemples : Tokens français cibles (aléatoires pour démo)\n",
    "fr_sentences = torch.randint(1, fr_vocab_size, (2, 6))  # Cible plus longue\n",
    "\n",
    "# Masque causal\n",
    "tgt_len = fr_sentences.size(1)\n",
    "self_mask = create_causal_mask(tgt_len).unsqueeze(0).unsqueeze(1).expand(2, num_heads, -1, -1)  # Ajusté\n",
    "\n",
    "# Forward : Génération avec cross-attention sur encoder_out (de Cellule 1)\n",
    "decoder_out = decoder(fr_sentences, encoder_out, self_mask)\n",
    "\n",
    "print(\"\\n=== Test Decoder + Génération Traduction ===\")\n",
    "print(f\"French target shape: {fr_sentences.shape}\")\n",
    "print(f\"Decoder output shape: {decoder_out.shape}\")\n",
    "print(f\"Exemple decoder_out[0, 0, :5]: {decoder_out[0, 0, :5]}\")\n",
    "print(\"✓ Decoder génère français à partir d'encoder anglais ! (aléatoire sans entraînement)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f099c3a-5474-4ae9-9b9e-5e45b6dce4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Average Loss: 4.5281\n",
      "Epoch 2/10, Average Loss: 3.7747\n",
      "Epoch 3/10, Average Loss: 3.4055\n",
      "Epoch 4/10, Average Loss: 3.0040\n",
      "Epoch 5/10, Average Loss: 2.6981\n",
      "Epoch 6/10, Average Loss: 2.3240\n",
      "Epoch 7/10, Average Loss: 2.0172\n",
      "Epoch 8/10, Average Loss: 1.7378\n",
      "Epoch 9/10, Average Loss: 1.4866\n",
      "Epoch 10/10, Average Loss: 1.2587\n",
      "Epoch 11/10, Average Loss: 1.0354\n",
      "Epoch 12/10, Average Loss: 0.8614\n",
      "Epoch 13/10, Average Loss: 0.7927\n",
      "Epoch 14/10, Average Loss: 0.7021\n",
      "Epoch 15/10, Average Loss: 0.5905\n",
      "Epoch 16/10, Average Loss: 0.5311\n",
      "Epoch 17/10, Average Loss: 0.4392\n",
      "Epoch 18/10, Average Loss: 0.3617\n",
      "Epoch 19/10, Average Loss: 0.2824\n",
      "Epoch 20/10, Average Loss: 0.2498\n",
      "Epoch 21/10, Average Loss: 0.2034\n",
      "Epoch 22/10, Average Loss: 0.1995\n",
      "Epoch 23/10, Average Loss: 0.1502\n",
      "Epoch 24/10, Average Loss: 0.1151\n",
      "Epoch 25/10, Average Loss: 0.1073\n",
      "Epoch 26/10, Average Loss: 0.0803\n",
      "Epoch 27/10, Average Loss: 0.0810\n",
      "Epoch 28/10, Average Loss: 0.0704\n",
      "Epoch 29/10, Average Loss: 0.0505\n",
      "Epoch 30/10, Average Loss: 0.0558\n",
      "✓ Entraînement sur données réelles terminé (10 époques) !\n",
      "Modèle sauvegardé en 'real_paragraph_translation_sentiment_model.pth'\n",
      "\n",
      "=== Test Sortie ===\n",
      "Phrase EN: I love reading books I love reading books\n",
      "Sentiment: Positive\n",
      "Traduction Générée FR: J'aime\n",
      "✓ Test OK !\n"
     ]
    }
   ],
   "source": [
    "# Cellule 3: Modèle Seq2Seq Complet + Entraînement sur Données Réelles Minimales (Petit Paragraphe) + 10 Époques + Sauvegarde (CORRIGÉ)\n",
    "# Correction: Vocab étendu pour inclure tous les indices (jusqu'à 38 pour FR). Données ajustées pour indices valides.\n",
    "# Ajout: Décodeurs pour afficher mots EN/FR. Sortie: Phrase EN, Sentiment, Traduction Générée (mots).\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# (Assumons classes précédentes définies : TranslationWithSentiment, create_causal_mask, etc.)\n",
    "\n",
    "# Vocab étendu pour couvrir tous les tokens (EN jusqu'à 32, FR jusqu'à 38)\n",
    "en_vocab = {\n",
    "    '<pad>': 0, '<sos>': 1, '<eos>': 2, 'I': 3, 'love': 4, 'reading': 5, 'books': 6, 'They': 7, 'transport': 8,\n",
    "    'me': 9, 'to': 10, 'other': 11, 'worlds': 12, 'The': 13, 'characters': 14, 'feel': 15, 'real': 16,\n",
    "    'Stories': 17, 'inspire': 18, 'But': 19, 'dislike': 20, 'long': 21, 'novels': 22, 'take': 23, 'too': 24,\n",
    "    'much': 25, 'time': 26, 'Short': 27, 'stories': 28, 'are': 29, 'better': 30, 'quick': 31, 'and': 32, 'engaging': 33\n",
    "}\n",
    "fr_vocab = {\n",
    "    '<pad>': 0, '<sos>': 1, '<eos>': 2, 'J\\'aime': 3, 'lire': 4, 'des': 5, 'livres': 6, 'Ils': 7, 'me': 8,\n",
    "    'transportent': 9, 'dans': 10, 'd\\'autres': 11, 'mondes': 12, 'Les': 13, 'personnages': 14, 'semblent': 15,\n",
    "    'réels': 16, 'histoires': 17, 'm\\'inspirent': 18, 'Mais': 19, 'je': 20, 'n\\'aime': 21, 'pas': 22,\n",
    "    'les': 23, 'romans': 24, 'longs': 25, 'prennent': 26, 'trop': 27, 'de': 28, 'temps': 29, 'récits': 30,\n",
    "    'courts': 31, 'sont': 32, 'mieux': 33, 'rapides': 34, 'et': 35, 'captivants': 36\n",
    "}\n",
    "\n",
    "en_vocab_size = len(en_vocab)  # 34\n",
    "fr_vocab_size = len(fr_vocab)  # 37\n",
    "d_model = 64\n",
    "num_heads = 2\n",
    "d_ff = 128\n",
    "num_layers = 1\n",
    "\n",
    "# Données réelles corrigées : Indices valides (max EN=32 -> 'stories', ajusté)\n",
    "train_data = [\n",
    "    # Positive\n",
    "    ([3, 4, 5, 6], [3, 4, 5, 6, 2], 1),  # \"I love reading books\" -> \"J'aime lire des livres\"\n",
    "    ([7, 8, 9, 10, 11, 12], [7, 8, 9, 10, 11, 12, 2], 1),  # \"They transport me to other worlds\"\n",
    "    ([13, 14, 15, 16], [13, 14, 15, 16, 2], 1),  # \"The characters feel real\"\n",
    "    ([17, 18, 3], [17, 18, 2], 1),  # \"Stories inspire me\" (ajusté FR pour 'm\\'inspirent')\n",
    "    # Negative/Mixed\n",
    "    ([19, 3, 20, 21, 22], [19, 20, 21, 22, 23, 24, 2], 0),  # \"But I dislike long novels\" -> \"Mais je n'aime pas les romans longs\"\n",
    "    ([7, 23, 24, 25, 26], [7, 26, 27, 28, 29, 2], 0),  # \"They take too much time\" -> \"Ils prennent trop de temps\"\n",
    "    ([27, 28, 29, 30], [30, 31, 32, 33, 2], 0),  # \"Short stories are better\" -> \"Récits courts sont mieux\"\n",
    "    ([7, 29, 31, 32, 33], [7, 32, 34, 35, 36, 2], 1),  # \"They are quick and engaging\" -> \"Ils sont rapides et captivants\"\n",
    "]\n",
    "\n",
    "# Décodeurs simples pour afficher mots\n",
    "en_rev_vocab = {v: k for k, v in en_vocab.items()}\n",
    "fr_rev_vocab = {v: k for k, v in fr_vocab.items()}\n",
    "\n",
    "def decode_tokens(tokens, rev_vocab):\n",
    "    words = [rev_vocab.get(t.item(), '<UNK>') for t in tokens if t.item() not in [0, 1, 2]]\n",
    "    return ' '.join(words)\n",
    "\n",
    "class MinimalDataset(Dataset):\n",
    "    def __init__(self, data, max_len=12):  # Réduit max_len\n",
    "        self.data = data\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        en_tokens, fr_tokens, sent_label = self.data[idx]\n",
    "        en_padded = torch.tensor(en_tokens + [0] * (self.max_len - len(en_tokens)))\n",
    "        fr_input_tokens = [1] + fr_tokens[:-1]\n",
    "        fr_input = torch.tensor(fr_input_tokens + [0] * (self.max_len - len(fr_input_tokens)))\n",
    "        fr_target = torch.tensor(fr_tokens + [0] * (self.max_len - len(fr_tokens)))\n",
    "        sent_label = torch.tensor(sent_label)\n",
    "        return en_padded, fr_input, fr_target, sent_label\n",
    "\n",
    "dataset = MinimalDataset(train_data)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Modèle\n",
    "device = torch.device('cpu')\n",
    "model = TranslationWithSentiment(en_vocab_size, fr_vocab_size, d_model, num_heads, d_ff, num_layers).to(device)\n",
    "\n",
    "# Loss et Optimizer\n",
    "trans_criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "sent_criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Entraînement : 10 époques\n",
    "model.train()\n",
    "for epoch in range(30):\n",
    "    total_loss = 0\n",
    "    for en_batch, fr_input, fr_target, sent_batch in dataloader:\n",
    "        en_batch, fr_input, fr_target, sent_batch = [t.to(device) for t in [en_batch, fr_input, fr_target, sent_batch]]\n",
    "        \n",
    "        tgt_mask = create_causal_mask(fr_input.size(1)).unsqueeze(0).unsqueeze(1).expand(1, num_heads, -1, -1).to(device)\n",
    "        trans_logits, sent_logits = model(en_batch, fr_input, tgt_mask)\n",
    "        \n",
    "        trans_loss = trans_criterion(trans_logits.view(-1, fr_vocab_size), fr_target.view(-1))\n",
    "        sent_loss = sent_criterion(sent_logits, sent_batch)\n",
    "        loss = trans_loss + sent_loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}/10, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"✓ Entraînement sur données réelles terminé (10 époques) !\")\n",
    "\n",
    "# Sauvegarde\n",
    "torch.save(model.state_dict(), 'real_paragraph_translation_sentiment_model.pth')\n",
    "print(\"Modèle sauvegardé en 'real_paragraph_translation_sentiment_model.pth'\")\n",
    "\n",
    "# Test post-entraînement avec affichage mots/sentiment/traduction\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Test sur première phrase positive\n",
    "    test_en_tokens = torch.tensor([[3, 4, 5, 6, 0] * 2]).to(device)  # \"I love reading books\" padded\n",
    "    en_phrase = decode_tokens(test_en_tokens[0], en_rev_vocab)\n",
    "    generated, sent_pred = model.translate(test_en_tokens, max_len=6, sos_token=1, eos_token=2)\n",
    "    gen_fr_tokens = torch.tensor(generated[0]).unsqueeze(0).to(device)\n",
    "    fr_translation = decode_tokens(gen_fr_tokens[0], fr_rev_vocab)\n",
    "    \n",
    "    print(f\"\\n=== Test Sortie ===\")\n",
    "    print(f\"Phrase EN: {en_phrase}\")\n",
    "    print(f\"Sentiment: {'Positive' if sent_pred == 1 else 'Negative'}\")\n",
    "    print(f\"Traduction Générée FR: {fr_translation}\")\n",
    "    print(\"✓ Test OK !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3ae054e-9da7-461b-97f0-fbd9af46e15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Modèle chargé depuis 'real_paragraph_translation_sentiment_model.pth'\n"
     ]
    }
   ],
   "source": [
    "# Cellule 1: Chargement du Modèle Sauvegardé et Préparation pour Génération Améliorée\n",
    "# Charge le modèle .pth sauvegardé. Ajoute un décodeur beam search simple pour meilleure génération.\n",
    "# Utilise les vocabs et classes précédentes.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# (Assumons vocabs et classes définies : en_vocab, fr_vocab, en_rev_vocab, fr_rev_vocab, TranslationWithSentiment, etc.)\n",
    "\n",
    "# Paramètres (mêmes que lors de l'entraînement)\n",
    "en_vocab_size = len(en_vocab)\n",
    "fr_vocab_size = len(fr_vocab)\n",
    "d_model = 64\n",
    "num_heads = 2\n",
    "d_ff = 128\n",
    "num_layers = 1\n",
    "\n",
    "# Chargement du modèle\n",
    "device = torch.device('cpu')\n",
    "model = TranslationWithSentiment(en_vocab_size, fr_vocab_size, d_model, num_heads, d_ff, num_layers).to(device)\n",
    "model.load_state_dict(torch.load('real_paragraph_translation_sentiment_model.pth', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "print(\"✓ Modèle chargé depuis 'real_paragraph_translation_sentiment_model.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85800292-b205-4832-afe2-302088d49c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Fonctions de génération améliorée et décodage prêtes ! (Shape tgt corrigée)\n"
     ]
    }
   ],
   "source": [
    "# Cellule 2: Fonction de Génération Améliorée avec Beam Search Simple et Décodeur de Texte (CORRIGÉ)\n",
    "# Correction: tgt shape [1, seq_len] pour tokens (pas [1, seq_len, 1]). Mask ajusté pour compatibilité.\n",
    "\n",
    "def improved_translate(model, src_en, beam_width=3, max_len=10, sos_token=1, eos_token=2, device='cpu'):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        encoder_out, sent_logits = model.encoder_clf(src_en.to(device))\n",
    "        sent_pred = sent_logits.argmax(dim=-1).item()\n",
    "        \n",
    "        # Beam search simple (batch=1 assumé)\n",
    "        beams = [(torch.tensor([sos_token]), 0.0)]  # (sequence [seq_len], score)\n",
    "        for _ in range(max_len):\n",
    "            new_beams = []\n",
    "            for seq, score in beams:\n",
    "                tgt = seq.unsqueeze(0).to(device)  # [1, seq_len] tokens\n",
    "                tgt_len = tgt.size(1)\n",
    "                self_mask = create_causal_mask(tgt_len).unsqueeze(0).unsqueeze(0).expand(1, num_heads, -1, -1).to(device)\n",
    "                dec_out = model.decoder(tgt, encoder_out, self_mask)\n",
    "                next_logits = model.generator(dec_out[:, -1, :])  # [1, vocab]\n",
    "                topk_probs, topk_ids = next_logits.topk(beam_width, dim=-1)\n",
    "                for i in range(beam_width):\n",
    "                    new_seq = torch.cat([seq, topk_ids[0, i].unsqueeze(0)], dim=0)\n",
    "                    new_score = score + torch.log(topk_probs[0, i] + 1e-8)\n",
    "                    new_beams.append((new_seq, new_score))\n",
    "            # Top beam_width\n",
    "            beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "            if eos_token in beams[0][0]:\n",
    "                break\n",
    "        \n",
    "        best_seq = beams[0][0]\n",
    "        return best_seq.cpu().tolist(), sent_pred\n",
    "\n",
    "def decode_sentence(tokens, rev_vocab):\n",
    "    words = []\n",
    "    for t in tokens:\n",
    "        if t == 1: continue  # <sos>\n",
    "        if t == 2: break  # <eos>\n",
    "        if t == 0: continue  # <pad>\n",
    "        words.append(rev_vocab.get(t, '<UNK>'))\n",
    "    return ' '.join(words)\n",
    "\n",
    "print(\"✓ Fonctions de génération améliorée et décodage prêtes ! (Shape tgt corrigée)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f221328-72de-476d-af5b-f8271c59da02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Génération de Texte et Traduction sur Paragraphe Réel ===\n",
      "\n",
      "Phrase 1:\n",
      "  EN: <UNK> <UNK> <UNK> <UNK>\n",
      "  Sentiment: Positive\n",
      "  FR Générée: J'aime lire des livres\n",
      "\n",
      "Phrase 2:\n",
      "  EN: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "  Sentiment: Positive\n",
      "  FR Générée: Ils me transportent dans d'autres mondes\n",
      "\n",
      "Phrase 3:\n",
      "  EN: <UNK> <UNK> <UNK> <UNK>\n",
      "  Sentiment: Positive\n",
      "  FR Générée: Les personnages semblent réels\n",
      "\n",
      "Phrase 4:\n",
      "  EN: <UNK> <UNK> <UNK>\n",
      "  Sentiment: Positive\n",
      "  FR Générée: histoires m'inspirent\n",
      "\n",
      "Phrase 5:\n",
      "  EN: <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "  Sentiment: Negative\n",
      "  FR Générée: Mais je n'aime pas les romans\n",
      "\n",
      "Phrase 6:\n",
      "  EN: <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "  Sentiment: Negative\n",
      "  FR Générée: Ils prennent trop de temps\n",
      "\n",
      "Phrase 7:\n",
      "  EN: <UNK> <UNK> <UNK> <UNK>\n",
      "  Sentiment: Negative\n",
      "  FR Générée: récits courts sont mieux\n",
      "\n",
      "Phrase 8:\n",
      "  EN: <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "  Sentiment: Positive\n",
      "  FR Générée: Ils sont rapides et captivants\n",
      "\n",
      "✓ Génération et traduction complètes affichées ! (Beam search corrigé)\n"
     ]
    }
   ],
   "source": [
    "# Cellule 3: Test sur un Petit Paragraphe Réel - Génération de Texte et Traduction Complète (CORRIGÉ)\n",
    "# Input: Paragraphe EN tokenisé. Sortie: Phrases EN, Sentiments, Traductions FR générées (mots lisibles).\n",
    "\n",
    "# Paragraphe exemple tokenisé (basé sur train_data, concaténé pour test)\n",
    "paragraph_en = [\n",
    "    [3, 4, 5, 6],  # \"I love reading books\"\n",
    "    [7, 8, 9, 10, 11, 12],  # \"They transport me to other worlds\"\n",
    "    [13, 14, 15, 16],  # \"The characters feel real\"\n",
    "    [17, 18, 3],  # \"Stories inspire me\"\n",
    "    [19, 3, 20, 21, 22],  # \"But I dislike long novels\"\n",
    "    [7, 23, 24, 25, 26],  # \"They take too much time\"\n",
    "    [27, 28, 29, 30],  # \"Short stories are better\"\n",
    "    [7, 29, 31, 32, 33]  # \"They are quick and engaging\"\n",
    "]\n",
    "\n",
    "print(\"=== Génération de Texte et Traduction sur Paragraphe Réel ===\")\n",
    "with torch.no_grad():\n",
    "    for i, en_tokens in enumerate(paragraph_en):\n",
    "        # Pad à max_len=12\n",
    "        padded_en = torch.tensor([en_tokens + [0] * (12 - len(en_tokens))]).to(device)\n",
    "        en_text = decode_sentence(padded_en[0], en_rev_vocab)\n",
    "        \n",
    "        gen_tokens, sent_pred = improved_translate(model, padded_en, beam_width=3, max_len=8)\n",
    "        fr_text = decode_sentence(gen_tokens, fr_rev_vocab)\n",
    "        sentiment = 'Positive' if sent_pred == 1 else 'Negative'\n",
    "        \n",
    "        print(f\"\\nPhrase {i+1}:\")\n",
    "        print(f\"  EN: {en_text}\")\n",
    "        print(f\"  Sentiment: {sentiment}\")\n",
    "        print(f\"  FR Générée: {fr_text}\")\n",
    "\n",
    "print(\"\\n✓ Génération et traduction complètes affichées ! (Beam search corrigé)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513904f9-9a16-4641-8e85-9666b611b954",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
